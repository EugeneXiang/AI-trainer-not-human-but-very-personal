# ğŸ§  AI Trainer Portfolio â€“ Eugene Xiang

> **"I don't train people to use AI. I train AI to understand people."**

Welcome to my digital lab, where prompts are recipes, LoRA is the seasoning, and punctuation monsters occasionally need to be tamed. This is not your average AI portfolioâ€”this is a record of **training the untrainable**, with a personality.

---

## ğŸ¯ About This Repository
This is the living portfolio of an AI Trainerâ€”not for humans, but for models. I'm Eugene Xiang, a business trainer-turned-model-whisperer who fine-tunes language models to reflect tone, reduce nonsense, and yes, stop them from abusing quotation marks.

I come from a background in corporate consulting, sales enablement, and digital transformationâ€”but these days, you'll find me poking around token distributions, designing prompt experiments, and whispering sweet syntax into GPT's ears.


---

## ğŸ§ª Project Modules

### 1. Prompt Engineering Lab
- Prompt iterations & behavior mapping
- Temperature and instruction-response experiments
- Use cases: business training, customer flow, internal knowledge base

### 2. LoRA å¾®è°ƒå®éªŒå®¤
- GPT-2 Small & Medium fine-tuning trials
- Dataset design & cleaning (including symbolic debugging)
- Loss curve tracking & model personality shifting

### 3. Punctuation Monster Chronicles
- Case: Comma loops, quote spirals, parentheses addiction
- Diagnosis process via token inspection
- Fix methods & post-fine-tune comparison

### 4. Custom Dataset: Netshop SQLite Whisper
- Real product dataset ingestion
- Prompt tuning to reflect e-commerce tone
- Plan to LoRA-train on structured table text pairs

### 5. æ™éœ†äººæ ¼æ¨¡å—å®éªŒ
- Designing emotional response styles
- Prompt-only vs fine-tuned behavior comparison
- Ethics of injecting personality into models

---

## ğŸ› ï¸ Stack & Tools
- Python, PyTorch, HuggingFace Transformers
- LoRA / PEFT (Parameter-Efficient Fine-Tuning)
- Tokenizer playgrounds (e.g., `tiktoken`, `BPE` tools)
- SQLite / Markdown prompt corpus building
- MPS on MacBook M3 + loss optimization gymnastics

---

## ğŸ§µ Linked Explorations
- [LinkedIn: Prompting the punctuation monster](#) (to be added)
- [Medium: I fine-tuned a GPT just to calm it down](#) (to be added)
- [æ™éœ† LoRA é£æ ¼åŒ–å®éªŒæ—¥å¿—](#)ï¼ˆto be addedï¼‰

---

## ğŸ§­ Philosophy Notes
> "Prompting is persuasion. Fine-tuning is memory surgery."

In this section Iâ€™ll drop occasional notes on:
- Prompting vs fine-tuning: when to use which
- What makes a model response feel â€œauthenticâ€?
- Where does understanding end and mimicry begin?

Stay tuned, this space evolves with every symbol tamed.
